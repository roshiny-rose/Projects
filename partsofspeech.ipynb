{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# Download and prepare the Penn Treebank dataset\n",
        "nltk.download(\"treebank\")\n",
        "from nltk.corpus import treebank\n",
        "\n",
        "# Load the dataset\n",
        "sentences = treebank.sents()\n",
        "\n",
        "# Load NLTK's pre-trained POS tagger\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"averaged_perceptron_tagger\")\n",
        "from nltk import pos_tag\n",
        "\n",
        "# Tokenize and tag the sentences using NLTK's POS tagger\n",
        "tagged_sentences = [pos_tag(sentence) for sentence in sentences]\n",
        "\n",
        "# Create a vocabulary and POS tag set\n",
        "words = [word for sentence in tagged_sentences for word, _ in sentence]\n",
        "tags = [tag for sentence in tagged_sentences for _, tag in sentence]\n",
        "word_to_idx = {word: idx for idx, word in enumerate(set(words), 1)}\n",
        "word_to_idx['<PAD>'] = 0\n",
        "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
        "pos_to_idx = {tag: idx for idx, tag in enumerate(set(tags))}\n",
        "idx_to_pos = {idx: tag for tag, idx in pos_to_idx.items()}\n",
        "\n",
        "# Convert words and tags to numerical values\n",
        "X = [[word_to_idx[word] for word, _ in sentence] for sentence in tagged_sentences]\n",
        "y = [[pos_to_idx[tag] for _, tag in sentence] for sentence in tagged_sentences]\n",
        "\n",
        "# Pad sequences to have the same length\n",
        "X_padded = pad_sequence([torch.LongTensor(sentence) for sentence in X], batch_first=True, padding_value=0)\n",
        "y_padded = pad_sequence([torch.LongTensor(tags) for tags in y], batch_first=True, padding_value=0)\n",
        "\n",
        "# Define the LSTM model\n",
        "class POSModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(POSModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(embedded)\n",
        "        output = self.fc(lstm_out)\n",
        "        return output\n",
        "\n",
        "# Initialize the model and set hyperparameters\n",
        "input_size = len(word_to_idx)\n",
        "hidden_size = 128\n",
        "output_size = len(pos_to_idx)\n",
        "model = POSModel(input_size, hidden_size, output_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Reduced training epochs and batch size\n",
        "num_epochs = 10\n",
        "batch_size = 16\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for i in range(0, len(X_padded), batch_size):\n",
        "        batch_x = X_padded[i:i + batch_size]\n",
        "        batch_y = y_padded[i:i + batch_size]\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_x)\n",
        "        loss = criterion(outputs.view(-1, output_size), batch_y.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 2)\n",
        "        correct += (predicted == batch_y).sum().item()\n",
        "        total_samples += batch_x.size(0) * batch_x.size(1)\n",
        "\n",
        "    accuracy = (correct / total_samples) * 100\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# Function to perform POS tagging on a given sentence\n",
        "def pos_tag_sentence(sentence):\n",
        "    tokens = nltk.word_tokenize(sentence)\n",
        "    tagged_tokens = pos_tag(tokens)\n",
        "    input_indices = [word_to_idx.get(token, 0) for token, _ in tagged_tokens]\n",
        "    input_tensor = torch.LongTensor(input_indices).unsqueeze(0)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        output = model(input_tensor)\n",
        "        _, predicted = torch.max(output, 2)\n",
        "        predicted_tags = [idx_to_pos[idx] for idx in predicted.squeeze().tolist()]\n",
        "        return list(zip(tokens, predicted_tags))\n",
        "\n",
        "# Input sentence\n",
        "input_sentence = \"As the sun began to set over the serene horizon, casting a warm, golden glow upon the tranquil waters of the lake, families gathered around campfires, sharing stories, laughter, and marshmallows, creating cherished memories that would be etched in their hearts forever.\"\n",
        "output_tags = pos_tag_sentence(input_sentence)\n",
        "print(\"POS Tags for Input Sentence:\")\n",
        "print(output_tags)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4w9jjJsKB8t",
        "outputId": "8e918414-b1bc-40a3-cf9c-60f4e5ee77d6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 81.3374, Accuracy: 93.30%\n",
            "Epoch [2/10], Loss: 31.8887, Accuracy: 96.50%\n",
            "Epoch [3/10], Loss: 21.4396, Accuracy: 97.64%\n",
            "Epoch [4/10], Loss: 15.5690, Accuracy: 98.28%\n",
            "Epoch [5/10], Loss: 11.9027, Accuracy: 98.69%\n",
            "Epoch [6/10], Loss: 9.3916, Accuracy: 98.96%\n",
            "Epoch [7/10], Loss: 7.5477, Accuracy: 99.16%\n",
            "Epoch [8/10], Loss: 6.1304, Accuracy: 99.32%\n",
            "Epoch [9/10], Loss: 5.0078, Accuracy: 99.44%\n",
            "Epoch [10/10], Loss: 4.1015, Accuracy: 99.55%\n",
            "POS Tags for Input Sentence:\n",
            "[('As', 'IN'), ('the', 'DT'), ('sun', 'DT'), ('began', 'CD'), ('to', 'TO'), ('set', 'VB'), ('over', 'IN'), ('the', 'DT'), ('serene', 'DT'), ('horizon', 'DT'), (',', ','), ('casting', 'JJ'), ('a', 'DT'), ('warm', 'DT'), (',', ','), ('golden', 'JJ'), ('glow', 'DT'), ('upon', 'NN'), ('the', 'DT'), ('tranquil', 'DT'), ('waters', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('lake', 'DT'), (',', ','), ('families', 'JJ'), ('gathered', 'DT'), ('around', 'IN'), ('campfires', 'DT'), (',', ','), ('sharing', 'DT'), ('stories', 'DT'), (',', ','), ('laughter', 'DT'), (',', ','), ('and', 'CC'), ('marshmallows', 'DT'), (',', ','), ('creating', 'NNP'), ('cherished', 'DT'), ('memories', 'NN'), ('that', 'IN'), ('would', 'MD'), ('be', 'VB'), ('etched', 'DT'), ('in', 'IN'), ('their', 'PRP$'), ('hearts', 'DT'), ('forever', 'DT'), ('.', '.')]\n"
          ]
        }
      ]
    }
  ]
}